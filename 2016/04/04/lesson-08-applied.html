<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Lesson 08 - Scikit-Learn</title>
  <meta name="description" content="Lesson 08 - Scikit-Learn">

  <link rel="stylesheet" href="/pythoncourse/css/main.css">
  <link rel="canonical" href="http://jeremy.kiwi.nz/pythoncourse/pythoncourse/2016/04/04/lesson-08-applied.html">
  <link rel="alternate" type="application/rss+xml" title="Python @ Precima" href="http://jeremy.kiwi.nz/pythoncourse/pythoncourse/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/pythoncourse/">Python @ Precima</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
          
          <a class="page-link" href="/pythoncourse/tag/Applied%20Statistics%20stream/">Applied Statistics stream</a>
          
        
          
          <a class="page-link" href="/pythoncourse/tag/R&D%20stream/">R&D stream</a>
          
        
      </div>
    </nav>

  </div>

</header>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73837623-1', 'auto');
  ga('send', 'pageview');

</script>

    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Lesson 08 - Scikit-Learn</h1>
    <p class="post-meta"><time datetime="2016-04-04T00:00:00-04:00" itemprop="datePublished">Apr 4, 2016</time> • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Jeremy</span></span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="lesson-08---scikit-learn">Lesson 08 - Scikit-Learn</h2>

<p>Welcome to lesson 9, on machine learning with scikit-learn.</p>

<p>Machine learning is a rapidly growing and advancing field, and the premier
module for carrying it out in Python is scikit-learn (aka scikits-learn). Today
we will cover the basic syntax and logic of the module, without touching too
deeply on the wide array of methods available.</p>

<p>Download the <a href="/pythoncourse/assets/notebooks/applied/lesson 08 applied.ipynb">notebook here</a></p>

<h3 id="machine-learning-101">Machine Learning 101</h3>

<p>We can define machine learning as building a program which can adapt to
previously seen data - we build an algorithm, which takes data as an input and
outputs predictions, once being trained on a training set.</p>

<p>We can broadly define machine learning into two major classes - Supervised and
Unsupervised.</p>

<p>In supervised clustering we have known targets - eg an iris species, or a
continuous variable such as retail spend for which we have both ‘features’, our
predictors, and ‘labels’, our outputs.</p>

<p>We can further subsection this into clustering, with categorical labels, and
regression, with continuous labels.</p>

<p>Similarly, we have unsupervised learning, in which we have features, but need to
discover or define the labels ourselves based on the data. Again, this splits
into clustering and regression. We can also often use unsupervised learning as a
dimensional reduction tool - PCA can be seen as a sort of unsupervised learning.</p>

<p>There can also be some middle area between the methods - maybe we wish to
validate our labels which we already have, or develop new ones.</p>

<h3 id="scikit-learn-101">Scikit-Learn 101</h3>

<p>Sklearn aims to be a unified module for all methods of machine learning, from
data splitting, normalization and reduction through to model fitting, prediction
and validation. The module is updated regularly to implement the most recent
machine learning algorithms. <a href="http://scikit-learn.org/stable/">The website</a> is a
great resource for help, examples and explanations of the differing methods we
can use.</p>

<p>sklearn does it best to implement the models in a uniform fashion, so we can
pass in the same data, and use the same transform, fit, predict, score and other
methods. This allows for a uniform workflow, and no need to transform data
between formats for different models.</p>

<p>In sklearn, we have a standard data format for most algorithms input data:</p>

<p>A numpy array of dimensions ‘samples’ by ‘features’. By Default these are stored
as np.float64, but we can change this if desired. Additionally we can use the
sparse matrix class from scipy if we have very large (and sparse) matrices.</p>

<p>Categorical features must be dummified (see last lesson)</p>

<p><strong>In [86]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">6</span><span class="p">,:])</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>[[ 5.1  3.5  1.4  0.2]
 [ 4.9  3.   1.4  0.2]
 [ 4.7  3.2  1.3  0.2]
 [ 4.6  3.1  1.5  0.2]
 [ 5.   3.6  1.4  0.2]
 [ 5.4  3.9  1.7  0.4]]
</code></pre>
</div>

<p>Addtionally we have a target, or label array if we have a supervised learning
dataset:</p>

<p><strong>In [87]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">6</span><span class="p">])</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>[0 0 0 0 0 0]
</code></pre>
</div>

<p><strong>In [88]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>['setosa' 'versicolor' 'virginica']
</code></pre>
</div>

<p>Feature engineering is one of the most important parts of machine learning and
requires knowledge of the dataset. We will not cover non-automated feature
engineering today.</p>

<p><img src="/pythoncourse/assets/sklearn/supervised.png" alt="png" />
<img src="/pythoncourse/assets/sklearn/supervised.png" alt="png" />
both from http://www.astroml.org/sklearn_tutorial/general_concepts.html</p>

<p>Overfitting is a serious problem in machine learning models - recall the problem
with <a href="http://www.wired.com/2015/10/can-learn-epic-failure-
google-flu-trends/">googles flu trends</a>. It ended up predicting winter, rather than flu. Similarly,
we might make a model that is really good at describing our current data, but
extremely poor at predicting the labels for new data.</p>

<p>We can stop overfitting in several ways. The first is to train our predictive
models on a training dataset, then test them on a held out, test set. This test
set should never be used for fitting models! We should also use simpler models,
and cross validation.</p>

<p>We have set ups for this in sklearn!</p>

<h3 id="workflow">Workflow</h3>

<p>We will work through the sklearn workflow step by step.</p>

<ul>
  <li>split our data into test and train sets</li>
  <li>Preprocess our data - extract features, dummyify, normalise, dimension
reduction</li>
  <li>Choose a model or models for our data</li>
  <li>Fit the models to our data</li>
  <li>Analyse our models output</li>
  <li>Validate our data on our test set and determine error</li>
  <li>Predict new data</li>
</ul>

<h3 id="splitting-data">Splitting data</h3>

<p>The <a href="http://scikit-
learn.org/stable/modules/cross_validation.html">cross validation submodule</a> is made to help avoid
overfitting. The most common method is to hold some of the data out of the
analysis, fit and then cimpare estimators on the test set.</p>

<p>Alternatively, the submodule contains methods for carrying out crossvalidation
during fitting, these should be used with caution.</p>

<p><strong>In [89]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="k">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>\
   <span class="n">train_test_split</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>(150, 4)
(90, 4)
(60, 4)
</code></pre>
</div>

<p>The function is smart, choosing to stratify within outcomes, and by default
holds out 25% of the data. If we have time series or other structured data, a
more complex scheme needs to be devised.</p>

<p>The test set should now not be touched until it’s time for validation</p>

<h3 id="preprocessing-data">Preprocessing data</h3>

<p>The next step is feature addition. This is the point where you can calculate
your features. Many features can be created from the data (recall the work we
did during the test). In general, it is best to carry out these operations with
the test data removed, so that the process is the same for any new data.</p>

<p>Depending on your choice of method, remove correlated features.</p>

<p>Once this is carried out, we can use the <a href="http://scikit-
learn.org/stable/modules/preprocessing.html#preprocessing">preprocessing
submodule</a> to process our data:</p>

<p><strong>In [90]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="c"># we can dummify data using http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</span>
<span class="c"># we can turn data into categories using http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html</span>
<span class="c"># see also imputer!</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="c">#be careful to use the correct version!</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">6</span><span class="p">,:])</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>[[ 0.18206758  0.71103882  0.45664061  0.5584109 ]
 [-1.17402201  0.00522823 -1.10334891 -1.19183221]
 [-0.04394735 -0.93585257  0.77939706  0.93346299]
 [-0.26996228 -0.93585257  0.29526238  0.1833588 ]
 [-0.26996228 -0.46531217 -0.02749407  0.1833588 ]
 [-0.38296975 -1.40639297  0.1876769   0.1833588 ]]
</code></pre>
</div>

<p>The next step to preprocess our data is optionally use matrix decomposition to
reduce the number of dimensions our data has. We can use the <a href="http://scikit-learn.org/stable/modules/decomposition.html">decomposition
module</a>:</p>

<p><strong>In [91]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c">#by fitting a model, we can transform our test data later using the transform method</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span></code></pre></figure>

<p>Now we can put our normalised and transformed data into a model</p>

<h3 id="choosing-and-fitting-a-model">Choosing and Fitting a Model</h3>

<p>Choosing a model is a statistical choice - I’ll leave it to you. You could even
<a href="http://scikit-
learn.org/stable/developers/contributing.html#rolling-your-own-estimator">make your own</a>.</p>

<p>Generally, we have a wide range of standard machine learning models - Let’s use
a <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a>
this time. Briefly, <a href="http://scikit-
learn.org/stable/modules/svm.html">an SVM draws</a> lines (or vector) between our classes of
data, either linearly or by apply the <a href="https://en.wikipedia.org/wiki/Kernel_method">‘kernel
trick’</a> to <a href="https://www.youtube.com/watch?v=3liCbRZPrZA">make it non-
linear</a>.</p>

<p>Again, as long as our data is in the correct format we can fit more or less any
model from sklearn.</p>

<p><strong>In [92]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="n">rbf_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">)</span>
<span class="c">#hyperparameters!</span>
<span class="c">#again, once a model is described, we need to fit it:</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">rbf_svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></figure>

<p><strong>In [93]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>array([1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0,
       2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0,
       0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1,
       1, 2, 2, 1, 2, 0, 1, 2, 2, 0, 1, 1, 2, 1, 0, 0, 0, 2, 1, 2, 0])
</code></pre>
</div>

<h3 id="evaluating-the-model">Evaluating the model</h3>

<p>Now we can evaluate our models, both on the training set, and on the test set.</p>

<p>We need to recreate our transformations on the test set….</p>

<p><strong>In [94]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#most models have a .score method, which is particular to them. It is normally 1 - out of classification rate</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>0.97777777777777775
</code></pre>
</div>

<p><strong>In [95]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,
       0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 0, 0, 1,
       2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2])
</code></pre>
</div>

<p><strong>In [96]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="c">#not quite as good, but much much better than the clustering we use last week</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>0.93333333333333335
</code></pre>
</div>

<p>We can also use the <a href="http://scikit-
learn.org/stable/modules/model_evaluation.html">metrics submodule</a> to do a ton of metrics about our
fit, or make our own:</p>

<p><strong>In [97]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Confusion matrix'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True label'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted label'</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span></code></pre></figure>

<p><img src="/pythoncourse/assets/sklearn/lesson-07---scikit-learn_21_0.png" alt="png" /></p>

<p>Now we can predict new data exactly as we did for our test set!</p>

<h3 id="saving-models">Saving Models</h3>

<p>We can save our model - a lot of algorithms will take a long time to train, and
the prediction is much faster, especially as it is run on a small amount of
newer data. To do this, we have to learn a little bit about pickling.</p>

<p>Briefly, we can see that python does not hold data as a tabular format in memory
- we can give arbitrary classes, so it has no natural state to be stored on the
disk. So far, every time we loaded or saved we used a csv or array, which is
naturally sequential.</p>

<p>Python has a built in module, called pickle, to save its native binary
representation of objects to disk. We will use the jobdumps module for now, as
it is faster for sklearn objects (it is optimised for numpy arrays).</p>

<p><strong>In [98]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.externals</span> <span class="kn">import</span> <span class="n">joblib</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">'model.pkl'</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>['model.pkl',
 'model.pkl_01.npy',
 'model.pkl_02.npy',
 'model.pkl_03.npy',
 'model.pkl_04.npy',
 'model.pkl_05.npy',
 'model.pkl_06.npy',
 'model.pkl_07.npy',
 'model.pkl_08.npy',
 'model.pkl_09.npy',
 'model.pkl_10.npy',
 'model.pkl_11.npy']
</code></pre>
</div>

<p>We can load it back in using</p>

<p><strong>In [99]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'model.pkl'</span><span class="p">)</span></code></pre></figure>

<h3 id="pipelines">Pipelines</h3>

<p>We can see that the method we used was fine, but reconstituting the pathway we
took through the processing was a little tricky for the test set. We need to
make sure we don’t miss a step, or our predictions will be off. We also needed
to make sure we used the model based transformations, as otherwise we couldn’t
apply the same transformations to our new data.</p>

<p>To make this easier to redo, we can use the <a href="http://scikit-
learn.org/stable/modules/pipeline.html">pipelines submodule</a>. Once we have a pipeline, we can call
fit and predict as though it were a single model.</p>

<p><strong>In [100]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c">#we get our estimators as a list of tuples:</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">[(</span><span class="s">'normalise'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()),</span>
              <span class="p">(</span><span class="s">'reduce_dim'</span><span class="p">,</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">()),</span>
              <span class="p">(</span><span class="s">'svm'</span><span class="p">,</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">))]</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">(</span><span class="n">estimators</span><span class="p">)</span></code></pre></figure>

<p><strong>In [101]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>\
   <span class="n">train_test_split</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>Pipeline(steps=[('normalise', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))])
</code></pre>
</div>

<p><strong>In [102]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>[2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0
 2 1 1 2 0 2 0 0 1 2 2 1 2 1 2 1 1 2 1 2 2 1 2]
0.933333333333
</code></pre>
</div>

<p>We can change the parameters in a pipeline. One of the reasons we might want to
to do this is to optimise a ‘hyperparameter’ one that does not depend on the
model.</p>

<p>We can do this using the <a href="http://scikit-
learn.org/stable/modules/grid_search.html">gridsearch submodule</a>:</p>

<p><strong>In [103]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c">#in the SVC, C is the penalty term, lambda is the degree of the kernel</span>
<span class="c">#we can change it using name__param</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">svm__C</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code>Pipeline(steps=[('normalise', StandardScaler(copy=True, with_mean=True, with_std=True)), ('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('svm', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))])
</code></pre>
</div>

<p>Grid search lets us search a range of parameters to find the best one:</p>

<p><strong>In [104]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#again using name__param. Can do as many as we'd like!</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'svm__C'</span><span class="p">:[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span></code></pre></figure>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="p">{</span><span class="err">'svm__C':</span><span class="w"> </span><span class="err">10</span><span class="p">}</span><span class="w">
</span><span class="mf">0.966666666667</span><span class="w">
</span><span class="mf">0.933333333333</span><span class="w">
</span></code></pre>
</div>

<p>There is still a ton more to learn from sklearn - we have not touch on ensemble
models, feature importance, roc curves, multithreading and a wide range of
models.</p>

<p>Now we know the basic syntax of split, transform, fit, predict, score we can run
most of the models in the module, and analyse their output. Further analysis is
left to you to read the docs and understand them.</p>

<p>For the rest of the lesson, we will work through the titanic dataset example,
fitted with a random forest model:
http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-
notebooks/blob/master/kaggle/titanic.ipynb</p>

  </div>
  
      <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'jeremycg';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  
</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Python @ Precima</h2>



  </div>

</footer>


  </body>

</html>
